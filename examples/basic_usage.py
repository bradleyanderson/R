#!/usr/bin/env python3
"""
ResolveAI Universal Platform - Basic Usage Examples

This file demonstrates the fundamental capabilities of ResolveAI.
Run these examples to get familiar with the platform.
"""

import asyncio
import os
from pathlib import Path

# Add the resolveai package to the path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from resolveai.core.universal_assistant import UniversalAssistant
from resolveai.config.settings import UniversalAssistantConfig


async def example_1_basic_startup():
    """Example 1: Basic startup and initialization."""
    print("ğŸš€ Example 1: Basic Startup")
    print("-" * 40)
    
    try:
        # Create basic configuration
        config = UniversalAssistantConfig(
            enable_screen_intelligence=True,
            enable_automation=True,
            enable_conversational_interface=True,
            debug=True
        )
        
        # Initialize assistant
        assistant = UniversalAssistant(config)
        
        # Start the assistant
        await assistant.start()
        
        print("âœ… Assistant started successfully!")
        print(f"ğŸŒ Web interface available at: http://localhost:{config.server_port}")
        
        return assistant
        
    except Exception as e:
        print(f"âŒ Failed to start assistant: {e}")
        return None


async def example_2_screen_analysis(assistant):
    """Example 2: Analyze the current screen."""
    print("\nğŸ” Example 2: Screen Analysis")
    print("-" * 40)
    
    try:
        # Analyze current screen
        analysis = await assistant.analyze_screen()
        
        print("ğŸ“Š Screen Analysis Results:")
        print(f"  â€¢ Interface Type: {analysis.get('interface_type', 'Unknown')}")
        print(f"  â€¢ UI Elements Found: {len(analysis.get('ui_elements', []))}")
        print(f"  â€¢ Text Regions: {len(analysis.get('text_regions', []))}")
        print(f"  â€¢ Confidence: {analysis.get('confidence', 0):.2f}")
        
        # Show detected elements
        if analysis.get('ui_elements'):
            print("\nğŸ¯ Detected UI Elements:")
            for i, element in enumerate(analysis['ui_elements'][:5]):  # Show first 5
                print(f"  {i+1}. {element.get('type', 'Unknown')} - {element.get('text', 'No text')}")
        
        return analysis
        
    except Exception as e:
        print(f"âŒ Screen analysis failed: {e}")
        return None


async def example_3_natural_language_control(assistant):
    """Example 3: Control software with natural language."""
    print("\nğŸ’¬ Example 3: Natural Language Control")
    print("-" * 40)
    
    try:
        # Example commands
        commands = [
            "What applications are currently open?",
            "Find all clickable buttons on the screen",
            "What text is visible in the current window?"
        ]
        
        for i, command in enumerate(commands, 1):
            print(f"\nğŸ—£ï¸ Command {i}: {command}")
            
            # Process the command
            response = await assistant.process_request({
                "type": "conversational",
                "input": command
            })
            
            print(f"ğŸ¤– Response: {response.get('response', 'No response')}")
            
    except Exception as e:
        print(f"âŒ Natural language control failed: {e}")


async def example_4_automation_workflow(assistant):
    """Example 4: Create and execute a workflow."""
    print("\nâš™ï¸ Example 4: Automation Workflow")
    print("-" * 40)
    
    try:
        # Define a simple workflow
        workflow = {
            "name": "Screen Capture Workflow",
            "description": "Take screenshot and analyze it",
            "steps": [
                {
                    "action": "capture_screen",
                    "params": {
                        "save_to": "workflow_screenshot.png"
                    }
                },
                {
                    "action": "analyze_screen",
                    "params": {
                        "screenshot_path": "workflow_screenshot.png"
                    }
                },
                {
                    "action": "log_results",
                    "params": {
                        "output_file": "workflow_results.txt"
                    }
                }
            ]
        }
        
        print("ğŸ“‹ Executing workflow:")
        print(f"  â€¢ Name: {workflow['name']}")
        print(f"  â€¢ Steps: {len(workflow['steps'])}")
        
        # Execute the workflow
        result = await assistant.execute_workflow(workflow)
        
        print(f"âœ… Workflow completed: {result.get('status', 'Unknown')}")
        
        if result.get('output_files'):
            print("ğŸ“ Generated files:")
            for file in result['output_files']:
                print(f"  â€¢ {file}")
                
    except Exception as e:
        print(f"âŒ Automation workflow failed: {e}")


async def example_5_plugin_system(assistant):
    """Example 5: Explore the plugin system."""
    print("\nğŸ”Œ Example 5: Plugin System")
    print("-" * 40)
    
    try:
        # List available plugins
        plugins = await assistant.list_plugins()
        
        print(f"ğŸ“¦ Available plugins: {len(plugins)}")
        for plugin in plugins:
            print(f"  â€¢ {plugin.get('name', 'Unknown')} v{plugin.get('version', '0.0.0')}")
            print(f"    {plugin.get('description', 'No description')}")
        
        # Show plugin capabilities
        if plugins:
            print(f"\nğŸ¯ Plugin capabilities:")
            for plugin in plugins[:3]:  # Show first 3
                capabilities = plugin.get('capabilities', [])
                print(f"  {plugin.get('name', 'Unknown')}: {', '.join(capabilities)}")
                
    except Exception as e:
        print(f"âŒ Plugin system exploration failed: {e}")


async def example_6_learning_and_adaptation(assistant):
    """Example 6: Demonstrate learning capabilities."""
    print("\nğŸ§  Example 6: Learning and Adaptation")
    print("-" * 40)
    
    try:
        # Get learning status
        learning_status = await assistant.get_learning_status()
        
        print("ğŸ“Š Learning System Status:")
        print(f"  â€¢ Learning Enabled: {learning_status.get('enabled', False)}")
        print(f"  â€¢ Patterns Learned: {learning_status.get('patterns_count', 0)}")
        print(f"  â€¢ User Preferences: {learning_status.get('preferences_count', 0)}")
        
        # Show recent learning activities
        if learning_status.get('recent_activities'):
            print("\nğŸ“ˆ Recent Learning Activities:")
            for activity in learning_status['recent_activities'][:3]:
                print(f"  â€¢ {activity.get('type', 'Unknown')}: {activity.get('description', 'No description')}")
        
        # Demonstrate preference learning
        print("\nğŸ¯ Teaching a preference...")
        await assistant.teach_preference({
            "context": "file_operations",
            "preference": "always_ask_before_deleting",
            "confidence": 0.9
        })
        
        print("âœ… Preference learned!")
        
    except Exception as e:
        print(f"âŒ Learning demonstration failed: {e}")


async def example_7_multi_modal_processing(assistant):
    """Example 7: Multi-modal data processing."""
    print("\nğŸ¨ Example 7: Multi-Modal Processing")
    print("-" * 40)
    
    try:
        # Test different data types
        test_files = [
            "examples/test_image.png",
            "examples/test_audio.mp3",
            "examples/test_document.pdf"
        ]
        
        for file_path in test_files:
            if os.path.exists(file_path):
                print(f"\nğŸ“ Processing: {file_path}")
                
                # Process the file
                result = await assistant.process_file(file_path)
                
                print(f"  â€¢ Type: {result.get('file_type', 'Unknown')}")
                print(f"  â€¢ Size: {result.get('file_size', 0)} bytes")
                print(f"  â€¢ Processing Time: {result.get('processing_time', 0):.2f}s")
                
                if result.get('extracted_content'):
                    content_preview = result['extracted_content'][:100] + "..." if len(result['extracted_content']) > 100 else result['extracted_content']
                    print(f"  â€¢ Content Preview: {content_preview}")
            else:
                print(f"âš ï¸ Test file not found: {file_path}")
        
        # Test cross-modal analysis
        print("\nğŸ”— Cross-Modal Analysis:")
        analysis = await assistant.analyze_relationships(test_files)
        
        print(f"  â€¢ Relationships Found: {len(analysis.get('relationships', []))}")
        for relationship in analysis.get('relationships', []):
            print(f"    {relationship.get('type', 'Unknown')}: {relationship.get('description', 'No description')}")
            
    except Exception as e:
        print(f"âŒ Multi-modal processing failed: {e}")


async def main():
    """Run all examples."""
    print("ğŸ¯ ResolveAI Universal Platform - Basic Usage Examples")
    print("=" * 60)
    print("This script demonstrates the core capabilities of ResolveAI.")
    print("Make sure you have configured your API keys first!")
    print("=" * 60)
    
    # Check configuration
    config_file = os.path.expanduser("~/.resolveai/config.yaml")
    if not os.path.exists(config_file):
        print("âš ï¸ Configuration not found. Please run: resolveai config --init")
        return
    
    # Start the assistant
    assistant = await example_1_basic_startup()
    if not assistant:
        print("âŒ Failed to start assistant. Exiting.")
        return
    
    try:
        # Run examples
        await example_2_screen_analysis(assistant)
        await example_3_natural_language_control(assistant)
        await example_4_automation_workflow(assistant)
        await example_5_plugin_system(assistant)
        await example_6_learning_and_adaptation(assistant)
        await example_7_multi_modal_processing(assistant)
        
        print("\nğŸ‰ All examples completed successfully!")
        print("\nğŸ“š Next Steps:")
        print("  â€¢ Open http://localhost:8000 for the web interface")
        print("  â€¢ Try the CLI: resolveai --help")
        print("  â€¢ Read the documentation: docs/")
        print("  â€¢ Join the community: discord.gg/resolveai")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Examples interrupted by user")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
    finally:
        # Clean up
        if assistant:
            await assistant.stop()
            print("\nğŸ›‘ Assistant stopped")


if __name__ == "__main__":
    asyncio.run(main())